---
title: Redis Architecture
date: "2018-02-02T22:22:22.169Z"
path:  "/redis-architecture"
tags:
   - middleware
   - redis
---
  
# Redis主从模式

缺点:
* 故障恢复复杂，如果没有RedisHA系统（需要开发），当主库节点出现故障时，需要**手动**将一个从节点晋升为主节点，
  同时需要通知客户端变更配置，并且需要让其它从库节点去复制新主库节点，整个过程需要人为干预，比较繁琐；

* 主库的写能力受到单机的限制，可以考虑分片；

* 主库的存储能力受到单机的限制，可以考虑Pika；

* 原生复制的弊端在早期的版本中也会比较突出，如：Redis复制中断后，Slave会发起psync，此时如果同步不成功，则会进行全量同步，
  主库执行全量备份的同时可能会造成毫秒或秒级的卡顿；又由于COW机制，导致极端情况下的主库内存溢出，程序异常退出或宕机；
  主库节点生成备份文件导致服务器磁盘IO和CPU（压缩）资源消耗；发送数GB大小的备份文件导致服务器出口带宽暴增，阻塞请求，建议升级到最新版本。

## Redis主从复制
在分布式系统中为了解决单点问题，通常会把数据复制多个副本部署到其他机器，满足故障恢复和负载均衡等需求。
Redis也是如此，它为我们提供了复制功能，实现了相同数据的多个Redis副本。复制功能是高可用Redis的基础

* 参与复制的Redis实例划分为主节点（master）和从节点（slave）。默认情况下，Redis都是主节点。每个从节点只能有一个主节点，
  通过配置或命令`slaveof {masterHost}{masterPort}`实现。
  而主节点可以同时具有多个从节点。复制的数据流是单向的，只能由主节点复制到从节点。
* 默认情况下，从节点使用slave-read-only=yes配置为只读模式。由于复制只能从主节点到从节点，对于从节点的任何修改主节点都无法感知，
  修改从节点会造成主从数据不一致
* 主从复制拓扑： 一主一从，一主多从(星状)，树状主从结构(链式)
* 可使用读写分离方案，降低Master节点的压力

### 主从节点模式下如何保持一致性
从Redis 2.8版本开始，可以配置主服务器连接N个以上从服务器才允许对主服务器进行写操作。
但是，因为Redis使用的是异步主从复制，没办法确保从服务器确实收到了要写入的数据，
所以还是有一定的数据丢失的可能性。

这一特性的工作原理如下：
* 从服务器每秒钟ping一次主服务器，确认处理的复制流数量。
* 主服务器记住每个从服务器最近一次ping的时间。
* 用户可以配置最少要有N个服务器有小于M秒的确认延迟。

如果有N个以上从服务器，并且确认延迟小于M秒，主服务器接受写操作。

你可以把这看做是CAP原则（一致性，可用性，分区容错性）不严格的**一致性**实现，
虽然不能百分百确保一致性，但至少保证了丢失的数据不会超过M秒内的数据量。

如果条件不满足，主服务器会拒绝写操作并返回一个错误。
* min-slaves-to-write（最小从服务器数）
* min-slaves-max-lag（从服务器最大确认延迟）


## Redis哨兵模式(Sentinel)
Redis Sentinel是一个分布式架构，其中包含若干个Sentinel节点和Redis数据节点，每个Sentinel节点会对数据节点和其他Sentinel节点进行监控，
当发现节点不可达时，会对节点做下线标识，如果被标识的是主节点，它会与其他Sentinel节点进行协商，当大多数Sentinel节点都认为主节点不可达时，
选举出一个Sentinel节点来进行自动故障转移工作，同时会将这个变化实时通知给应用方
哨兵的功能
* 监控，定期检测数据节点和其他Sentinel节点
* 通知，通知故障转移的结果给应用方
* 主节点故障转移
* 配置提供者：客户端初始化时连接的时Sentinel节点集合，从中获取主节点信息

配置：`sentinel monitor <master-name> <ip> <port> <quorum>`

缺点: 资源浪费，Redis数据节点中slave节点作为备份节点不提供服务；不能解决读写分离问题，实现起来相对复杂


### 哨兵架构下的客户端连接
* 遍历Sentinel节点集合，找到一个可用的Sentinel节点
* 通过`sentinel get-master-addr-by-name master-name`这个api获得主节点信息
* 验证当前获取的主节点信息，防止故障转移时节点变化
* 为Sentinel节点单独启动一个线程，订阅Sentinel节点上切换master相关的channel信息

### 哨兵实现原理
哨兵的作用就是监控redis主、从数据库是否正常运行，主出现故障自动将从数据库转换为主数据库。

_监控_: Sentinel三个定时监控任务
1. 每隔10秒，每个Sentinel节点会向主节点和从节点发送info命令获取最新的*拓扑结构*
2. 每隔两秒，每个Sentinel节点会向Redis数据节点的`__sentinel__:hello`频道发送该Sentinel节点对于主节点的判断和当前的Sentinel信息。
3. 每隔1秒，每个Sentinel节点会向主节点、从节点和其余的Sentinel节点发送ping命令做心跳检测，确认节点是否可达

_判断主节点下线_: 注意从节点即使客观下线也不进行故障转移
* 主观下线，当前节点认为某节点下线；
* 客观下线：超过`quorum`个Sentinel节点认为某节点下线

_领导者Sentinel节点选举_ - Raft算法
大致思路（Redis开发与运维9.5.3）
1. 每个在线的Sentinel节点都有资格成为领导者，当它确认主节点主观下线时，会向其他Sentinel节点发送命令，要求将自己设置为领导者
2. 收到命令的Sentinel节点，如果没有同意过其他Sentinel节点的请求命令，则同意该请求，否则拒绝
3. 如果该Sentinel节点发现自己的票数大于等于`max(quorum, num(sentinels) / 2 + 1)`,那么它将成为领导者
4. 如果此过程没有选举出领导者，将进入下次选举

_Master与slave的切换_
1. slave leader升级为master
2. 其他slave修改为新master的slave
3. 通知客户端, 客户端修改连接
4. 老的master如果重启成功，变为新master的slave

* <https://www.cnblogs.com/xybaby/p/10124083.html>
* <https://www.cnblogs.com/mindwind/p/5231986.html>




## Redis集群模式 Cluster
即使使用哨兵，redis每个实例也是全量存储，每个redis存储的内容都是完整的数据，浪费内存且有木桶效应.
为了最大化利用内存，可以采用cluster群集，就是分布式存储。即每台redis存储不同的内容。
群集至少需要3主3从, redis-master节点一般用于接收读写，而redis-slave节点则一般**只用于备份**,
当有请求是在向slave发起时，会直接重定向到对应key所在的master来处理。
但如果不介意读取的是redis-cluster中有可能过期的数据并且对写请求不感兴趣时，则亦可通过readonly命令，
将slave设置成可读，然后通过slave获取相关的key，达到读写分离。

_优点_:
* 无中心架构；
* 数据按照slot存储分布在多个节点，节点间数据共享，可动态调整数据分布；
* 可扩展性：可线性扩展到1000多个节点，节点可动态添加或删除；
* 高可用性：部分节点不可用时，集群仍可用。通过增加Slave做standby数据副本，能够实现故障自动failover，
  节点之间通过gossip协议交换状态信息，用投票机制完成Slave到Master的角色提升；

_缺点_
* key批量操作支持有限。如mset、mget,目前只支持具有相同slot值的key执行批量操作
* key事务操作支持有限。同理只支持多key在同一节点上的事务操作,当多个key分布在不同的节点上时无法使用事务功能
* key作为数据分区的最小粒度,因此不能将一个大的键值对象如hash、list等映射到不同的节点
* 不支持多数据库空间。单机下的Redis可以支持16个数据库,集群模式下只能使用一个数据库空间,即db0
* 复制结构只支持一层,从节点只能复制主节点,不支持嵌套树状复制结构。
*　Client实现复杂，驱动要求实现Smart Client，缓存slots mapping信息并及时更新，提高了开发难度，客户端的不成熟影响业务的稳定性。
　目前仅JedisCluster相对成熟，异常处理部分还不完善，比如常见的“max redirect exception”。
*　节点会因为某些原因发生阻塞（阻塞时间大于clutser-node-timeout），被判断下线，这种failover是没有必要的。
* 数据通过异步复制，不保证数据的强一致性。
* 多个业务使用同一套集群时，无法根据统计区分冷热数据，资源隔离性较差，容易出现相互影响的情况。
* redis cluster默认是不支持slave节点读或者写的，Slave在集群中充当“冷备”，**不能缓解读压力**，当然可以通过SDK的合理设计来提高Slave资源的利用率。


### cluster 键分布模型
* Redis集群的键空间被分割为 16384 个槽（slot）， 集群的最大节点数量也是16384个。 Redis Cluser采用虚拟槽分区,所有的键根据哈希函数映射到0~16383整数槽内,
  计算公式:`slot=CRC16(key) & 16383`。每一个节点负责维护一部分槽以及槽所映射的键值数据
* 推荐的最大节点数量为 1000 个左右。每个主节点都负责处理 16384 个哈希槽的其中一部分。
* 当我们说一个集群处于“稳定”（stable）状态时， 指的是集群没有在执行重配（reconfiguration）操作，
  每个哈希槽都只由一个节点进行处理。重配置指的是将某个/某些槽从一个节点移动到另一个节点。
  一个主节点可以有任意多个从节点，这些从节点用于在主节点发生网络断线或者节点失效时， 对主节点进行替换。


### 节点握手
节点握手是指一批运行在集群模式下的节点通过Gossip(流言)协议彼此通信,达到感知对方的过程;
Gossip消息可分为:ping消息、pong消息、meet消息、fail消息 

### 集群伸缩
* 扩容集群
1) 准备新节点 2)加入集群, 3)迁移槽和数据

为保证整个集群的高可用,使用`cluster replicate{masterNodeId}`命令为主节点添加对应从节点,注意在集群模式下slaveof添加从节点操作不再支持

* 收缩集群
1.下线迁移槽 
2.忘记节点, 让其他节点不再与要下线节点进行Gossip消息交换,`cluster forget{downNodeId}`命令

### Redis 数据迁移 (主节点增加)
MIGRATE target_host target_port key target_database id timeout
* 该命令会将所指定的键原子地（atomic）从节点 A 移动到节点 B , 
  （在移动键期间，两个节点都会处于**阻塞**状态，以免出现竞争条件）

* 执行MIGRATE 命令的节点会连接到 target 节点， 
  并将序列化后的 key 数据发送给 target ， 一旦 target 返回 OK ,
  节点就将自己的 key 从数据库中删除。

* 从一个外部客户端的视角来看， 在某个时间点上， 键 key 要么存在于节点 A ， 
要么存在于节点 B ， 但不会同时存在于节点 A 和节点 B 。

* 因为 Redis集群只使用 0 号数据库， 所以当 MIGRATE 命令被用于执行集群操作时， 
  target_database 的值总是 0 。

### Redis Cluster - 请求路由
Redis集群对客户端通信协议做了比较大的修改,为了追求性能最大化,并没有采用代理的方式而是采用**客户端直连节点**的方式.
因此对于希望从单机切换到集群环境的应用*需要修改客户端代码*.

* 请求重定向: 在集群模式下,Redis接收任何键相关命令时首先计算键对应的槽,再根据槽找出所对应的节点,如果节点是自身,则处理键命令;
  否则回复MOVED重定向错误,通知客户端请求正确的节点
  1. 计算槽: 根据键的有效部分使用CRC16函数计算出散列值,再取对16383的余数, 
     如果键内容包含`{`和`}`大括号字符,则计算槽的**有效部分**是括号内的内容(hash_tag);否则采用键的全内容计算槽。
  2. 槽节点查找: 要查找槽所对应的节点。集群内通过消息交换每个节点都会知道所有节点的槽信息,内部保存在clusterState结构中

* Smart客户端: Smart客户端通过在内部维护slot→node的映射关系,本地就可实现键到节点的查找(不用随机发送到redis节点再根据MOVED重定向),从而保证IO效率的最大化,
  而MOVED重定向负责协助Smart客户端更新slot→node映射。Java的Jedis为例,说明Smart客户端操作集群的流程:
  1. JedisCluster初始化时会选择一个运行节点,初始化槽和节点映射关系
  2. JedisCluster解析cluster slots结果缓存在本地,并为每个节点创建唯一的JedisPool连接池。
  3. 键命令执行: 计算slot并根据slots缓存获取目标节点连接,发送命令,如果出现连接错误,使用**随机**连接重新执行键命令,
     捕获到MOVED重定向错误,使用cluster slots命令更新slots缓存;重复执行(最多redirections次),直到命令执行成功


## Redis in iQIYI

Redis 也是使用 master - slave 这种方式，由于网络的复杂性我们对 Sentinel 的部署进行了一些特殊配置，在多机房的情况下每个机房配置一定数量 Sentinel 来避免脑裂。

备份恢复方面介绍一个我们的特殊场景，虽然 Redis 是一个缓存，但我们发现不少的业务同学会把它当做一个 KVDB 来使用，在某些情况下会造成数据的丢失。所以我们做了一个 Redis 实时备份功能，启动一个进程伪装成 Redis 的 Slave 实时获取数据，再放到后端的 KV 存储里，例如 ScyllaDB，如果要恢复就可以从 ScyllaDB 里把数据拉出来。我们在用 Redis 时最大的痛点就是它对网络的延迟或抖动非常敏感。如有抖动造成 Redis Master 超时，会由 Sentinel 重新选出一个新的节点成为 Master，再把该节点上的数据同步到所有 Slave 上，此过程中数据会放在 Master 节点的 Buffer 里，如果写入的 QPS 很高会造成 Buffer 满溢。如果 Buffer 满后 RDB 文件还没有拷贝过去，重建过程就会失败。

基于这种情况，我们对 Redis 告警做了自动化优化，如有大量 master - slave 重建失败，我们会动态调整一些参数，例如把 Buffer 临时调大等， 此外我们还做了 Redis 集群的自动扩缩容功能。

我们在做 Redis 开发时如果是 Java 语言都会用到 Jedis。用 Jedis 访问客户端分片的 Redis 集群，如果某个分片发生了故障或者 failover，Jedis 就会对所有后端的分片重建连接。如果某一分片发生问题，整个 Redis 的访问性能和 QPS 会大幅降低。针对这个情况我们优化了 Jedis，如果某个分片发生故障，就只针对这个分片进行重建。

在业务访问 Redis 时我们会对 Master 绑定一个读写域名，多个从库绑定读域名。但如果我们进行 Master failover，会将读写域名从某旧 Master 解绑，再绑定到新 Master 节点上。DNS 本身有一个超时时间，所以数据库做完 failover 后业务程序里没有立刻获取到新的 Master 节点的 IP的话，有可能还会连到原来的机器上，造成访问失败。我们的解决方法是把 DNS 的 TTL 缩短，但对 DNS 服务又会造成很大的压力，所以我们在 SDK 上提供 Redis 的名字服务 RNS，RNS 从 Sentinel 里获取集群的拓扑和拓扑的变化情况，如果集群 failover，Sentinel 会接到通知，客户端就可以通过 RNS(redis name server) 来获取新的 Master 节点的 IP 地址。我们去掉域名，通过 IP 地址来访问整个集群，屏蔽了 DNS 的超时，缩短了故障的恢复时间。SDK 上还做了一些功能，例如 Load Balance 以及故障检测，比如某个节点延时较高的话会被临时熔断等。

客户端分片的方式会造成 Redis 的扩容非常痛苦，如果客户端已经进行了一定量的分片，之后再增加就会非常艰难。Redis 在 3.0 版本后会提供 Redis Cluster，因为功能受限在爱奇艺应用的不是很多，例如不支持显示跨 DC 部署和访问，读写只在主库上等。我们某些业务场景下会使用 Redis 集群，例如数据库访问只发生在本 DC，我们会在DC内部进行 Cluster 部署。但有些业务在使用的过程中还是想做 failover，如果集群故障可以切换到其他集群。根据这种情况我们做了一个 Proxy，读写都通过它来进行。写入数据时 Proxy 会做一个旁路，把新增的数据写在 Kafka 里，后台启用同步程序再把 Kafka 里的数据同步到其他集群，但存在一些限制，比如我们没有做冲突检测，所以集群间数据需要业务的同学做单元化。线上环境的Redis Cluster 集群间场景跨 DC 同步 需要 50 毫秒左右的时间。

[爱奇艺实用数据库选型树：不同场景如何快速选择数据库？](https://asktug.com/t/topic/1396)

## redis 自研关键点
* 不使用原生的redis cluster, 自行进行数据分片
* 基本采用sentinel模式, 实现了从节点的读功能, 使用ScyllaDB伪装成slave做成HA_Slave, 实现主节点的故障转移(failover)
* 自研Smart客户端, 加入`name service`层管理节点信息, 不直接使用Sentinel中的节点信息, 可实现读写分离
* Smart客户端带自动读写分离, 负载均衡和故障检测, 可实现临时熔断
* 告警做了自动化优化, 重建时动态调整配置参数


 
* [Redis系列文章](https://www.cnblogs.com/kismetv/p/9609938.html)
* [目前最全的Redis高可用技术解决方案总结](https://juejin.im/entry/5b7a27ade51d4538d5174b83)
* [redis架构演变与redis-cluster群集读写方案](https://cloud.tencent.com/developer/article/1187745)